{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3324f7",
   "metadata": {},
   "source": [
    "### üì¶ Step 1: Importing Libraries\n",
    "\n",
    "**What I'm doing here:**  \n",
    "I‚Äôm importing all the essential Python libraries I‚Äôll use in this notebook:\n",
    "- `pandas` and `numpy` for handling data,\n",
    "- `sentence-transformers` for semantic embeddings,\n",
    "- `cosine_similarity` from scikit-learn to measure similarity between texts,\n",
    "- `TfidfVectorizer` to extract features from text using TF-IDF,\n",
    "- `matplotlib.pyplot` for visualization,\n",
    "- and other helpers like `time`, `re`, and Hugging Face‚Äôs `AutoTokenizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485dbd1c",
   "metadata": {},
   "source": [
    "### üìÇ Step 2: Loading the Dataset\n",
    "\n",
    "**What I'm doing here:**  \n",
    "Now I'm loading the dataset (`DataNeuron_Text_Similarity.csv`) into a DataFrame using pandas.  \n",
    "I'll preview the first few rows to understand the structure of the data I'm working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Dataset\n",
    "file_path = \"DataNeuron_Text_Similarity.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2d9e8",
   "metadata": {},
   "source": [
    "# Dataset Understanding & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04992fc9",
   "metadata": {},
   "source": [
    "### üìñ Step 3: Measuring Readability\n",
    "\n",
    "**What I'm doing here:**  \n",
    "I'm using the `textstat` library to measure how easy it is to read each sentence using the **Flesch Reading Ease** score.  \n",
    "This helps me understand the complexity of the text in both `text1` and `text2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f2dd0",
   "metadata": {},
   "source": [
    "### Readability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "read_scores1 = df['text1'].apply(textstat.flesch_reading_ease)\n",
    "print(\"Avg Flesch Reading Score of text1:\", read_scores1.mean())\n",
    "read_scores2 = df['text2'].apply(textstat.flesch_reading_ease)\n",
    "print(\"Avg Flesch Reading Score of text2:\", read_scores2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bb862",
   "metadata": {},
   "source": [
    "### üö´ (Skipped) Optional Text Cleaning\n",
    "\n",
    "**What I planned here (but skipped):**  \n",
    "I had written a function to clean the text by converting it to lowercase, removing special characters, and stripping extra spaces.  \n",
    "But I‚Äôve commented it out because I'm working with already clean data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80354f82",
   "metadata": {},
   "source": [
    "### Clean & Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = str(text).lower()\n",
    "#     text = re.sub(r'[^a-z0-9]', ' ', text)\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#     return text\n",
    "\n",
    "# df['text1_clean'] = df['text1'].apply(clean_text)\n",
    "# df['text2_clean'] = df['text2'].apply(clean_text)\n",
    "\n",
    "# Will not be used here as we are using the pre-cleaned data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14e38d",
   "metadata": {},
   "source": [
    "### üî¢ Step 4: Analyzing Text Length\n",
    "\n",
    "**What I'm doing here:**  \n",
    "I'm calculating the number of words in `text1` and `text2`. This helps me understand how long the texts are on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d926b4",
   "metadata": {},
   "source": [
    "### Length Analysis (Word Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34324645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text1_len'] = df['text1'].str.split().str.len()\n",
    "df['text2_len'] = df['text1'].str.split().str.len()\n",
    "\n",
    "print(\"Average length of text1:\", df['text1_len'].mean())\n",
    "print(\"Average length of text2:\", df['text2_len'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce7d8c",
   "metadata": {},
   "source": [
    "### üßÆ Step 5: TF-IDF Baseline Similarity\n",
    "\n",
    "**What I'm doing here:**  \n",
    "Now I‚Äôm building a **baseline model** using TF-IDF vectorization to represent the texts as vectors.  \n",
    "Then I compute **cosine similarity** between the vectors of `text1` and `text2`.  \n",
    "This gives me a basic similarity score using traditional NLP techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f6995",
   "metadata": {},
   "source": [
    "# Baseline & Feature-Based Similarity\n",
    "Before embeddings, test traditional vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TF-IDF Baseline\n",
    "# Baseline using vectorization and cosine similarity (traditional method)\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf.fit(pd.concat([df['text1'], df['text2']]))\n",
    "\n",
    "t1_vecs = tfidf.transform(df['text1'])\n",
    "t2_vecs = tfidf.transform(df['text2'])\n",
    "\n",
    "df['similarity_tfidf'] = [cosine_similarity(t1_vecs[i], t2_vecs[i])[0][0] for i in range(len(df))]\n",
    "df[['text1', 'text2', 'similarity_tfidf']].head()\n",
    "print(\"üßÆ Baseline TF-IDF Avg Similarity:\", round(df['similarity_tfidf'].mean(), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742080b",
   "metadata": {},
   "source": [
    "# Chossing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b9856",
   "metadata": {},
   "source": [
    "\n",
    "Semantic Similarity Evaluation using Sentence Transformers\n",
    "-----------------------------------------------------------\n",
    "Assumptions\n",
    "Unlabeled dataset: No ground truth similarity scores provided.\n",
    "\n",
    "Can use pretrained models, embedding distances, or generate pseudo-labels for supervised fine-tuning.\n",
    "\n",
    "Focus: How semantically similar two paragraphs are, not just word overlap.\n",
    "\n",
    "Evaluation Metrics\n",
    "Cosine Similarity: Measures the cosine of the angle between two vectors.\n",
    "Average Cosine Similarity: Computes the average cosine similarity across all pairs.\n",
    "Runtime Performance\n",
    "\n",
    "This evaluates different sentence embedding models for their \n",
    "performance in measuring semantic similarity between pairs of texts. \n",
    "It computes average cosine similarity scores, runtime performance, \n",
    "and token usage per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49323a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Model Caching\n",
    "# -----------------------------\n",
    "_cached_models = {}\n",
    "_cached_tokenizers = {}\n",
    "\n",
    "def load_model_and_tokenizer(name, path):\n",
    "    if name not in _cached_models:\n",
    "        print(f\"\\nüîç Loading model and tokenizer for {name}...\")\n",
    "        _cached_models[name] = SentenceTransformer(path)\n",
    "        _cached_tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    return _cached_models[name], _cached_tokenizers[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Token Count Estimation\n",
    "# -----------------------------\n",
    "def get_token_count(tokenizer, texts, max_length=None):\n",
    "    \"\"\"\n",
    "    Estimate token count per text using the model tokenizer.\n",
    "    \"\"\"\n",
    "    counts = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, truncation=False)\n",
    "        if max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        counts.append(len(tokens))\n",
    "    return counts\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Text Truncation Helper\n",
    "# -----------------------------\n",
    "def truncate_texts(texts, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Truncate each text to max_length tokens.\n",
    "    \"\"\"\n",
    "    truncated_texts = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, truncation=False)\n",
    "        tokens = tokens[:max_length]\n",
    "        truncated = tokenizer.decode(tokens, clean_up_tokenization_spaces=True)\n",
    "        truncated_texts.append(truncated)\n",
    "    return truncated_texts\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Model Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate_model(name, model, tokenizer, texts1, texts2, max_length=None,batch_size=32):\n",
    "    \"\"\"\n",
    "    Run model evaluation: encode, compute similarity, runtime, and token stats.\n",
    "    \"\"\"\n",
    "    # Truncate input texts\n",
    "    if max_length:\n",
    "        texts1 = truncate_texts(texts1, tokenizer, max_length)\n",
    "        texts2 = truncate_texts(texts2, tokenizer, max_length)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Encode with normalization\n",
    "    emb1 = model.encode(texts1, convert_to_numpy=True, normalize_embeddings=True,show_progress_bar=True, truncation=True,batch_size=batch_size)\n",
    "    emb2 = model.encode(texts2, convert_to_numpy=True, normalize_embeddings=True,show_progress_bar=True, truncation=True,batch_size=batch_size)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = [cosine_similarity([emb1[i]], [emb2[i]])[0][0] for i in range(len(emb1))]\n",
    "    avg_sim = np.mean(similarities)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Token statistics\n",
    "    token_counts1 = get_token_count(tokenizer, texts1, max_length)\n",
    "    token_counts2 = get_token_count(tokenizer, texts2, max_length)\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"avg_similarity\": round(avg_sim, 4),\n",
    "        \"runtime_sec\": round(elapsed, 2),\n",
    "        \"avg_tokens_text1\": round(np.mean(token_counts1), 2),\n",
    "        \"avg_tokens_text2\": round(np.mean(token_counts2), 2)\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Model Configuration\n",
    "# -----------------------------\n",
    "model_configs = {\n",
    "    \"MiniLM-L12\": \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    \"MPNet\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"MultiQA\": \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Evaluation\n",
    "# -----------------------------\n",
    "def run_evaluation(df, sample_size=50, max_length=512):\n",
    "    sample = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "    results = []\n",
    "\n",
    "    for name, model_path in model_configs.items():\n",
    "        model, tokenizer = load_model_and_tokenizer(name, model_path)\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        res = evaluate_model(\n",
    "            name,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            sample['text1'].tolist(),\n",
    "            sample['text2'].tolist(),\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"avg_similarity\", ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8648d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(50, random_state=42).reset_index(drop=True)\n",
    "print(\"Sample of 50 rows from the dataset:\")\n",
    "print(\"Running model comparisons on text pairs...\")\n",
    "summary = run_evaluation(df)\n",
    "\n",
    "print(\"\\n Final Evaluation Summary:\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7bcee",
   "metadata": {},
   "source": [
    "After running the semantic similarity evaluation, I observed that MultiQA achieved the highest average similarity (0.1913), indicating better semantic understanding of longer texts, though it took the most time (~119s). MPNet offered a good balance between accuracy and speed, while MiniLM-L12 was by far the fastest (~5s), but with a trade-off in similarity score.\n",
    "\n",
    "Based on this, I chose MultiQA for its superior semantic accuracy, which is crucial for my use case where capturing deeper meaning is more important than runtime speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def compute_and_normalize_similarity(df, text_col1='text1', text_col2='text2', max_length=512):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity using MultiQA on full dataset and normalize results [0,1].\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with two text columns.\n",
    "        text_col1 (str): Name of first text column.\n",
    "        text_col2 (str): Name of second text column.\n",
    "        max_length (int): Max token length to truncate texts before encoding.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with new 'normalized_similarity' column.\n",
    "    \"\"\"\n",
    "    # Load MultiQA model and tokenizer (cached)\n",
    "    model, tokenizer = load_model_and_tokenizer(\"MultiQA\", model_configs[\"MultiQA\"])\n",
    "\n",
    "    # Optionally truncate texts\n",
    "    texts1 = truncate_texts(df[text_col1].tolist(), tokenizer, max_length)\n",
    "    texts2 = truncate_texts(df[text_col2].tolist(), tokenizer, max_length)\n",
    "\n",
    "    # Encode texts\n",
    "    emb1 = model.encode(texts1, convert_to_numpy=True, show_progress_bar=True)\n",
    "    emb2 = model.encode(texts2, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "    # Compute cosine similarity diagonal for each pair\n",
    "    similarities = np.diag(cosine_similarity(emb1, emb2))\n",
    "\n",
    "    # Normalize similarities to range [0,1]\n",
    "    scaler = MinMaxScaler()\n",
    "    similarities_norm = scaler.fit_transform(similarities.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Add normalized similarity column\n",
    "    df_Next = df.copy()\n",
    "    df_Next['normalized_similarity'] = similarities_norm\n",
    "\n",
    "    return df_Next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb0629",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "In this cell, I run the code above to advance my semantic modeling workflow. It helps me understand how the data is being transformed and prepares it for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your full dataset with columns 'text1' and 'text2'\n",
    "df_with_sim = compute_and_normalize_similarity(df, max_length=512)\n",
    "print(df_with_sim.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91157ac2",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "In this cell, I run the code above to advance my semantic modeling workflow. It helps me understand how the data is being transformed and prepares it for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1da3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyze Results\n",
    "print(\"\\nTop 5 Most Similar Pairs:\")\n",
    "df.sort_values('normalized_similarity', ascending=False).head(5)[['text1', 'text2', 'normalized_similarity']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635cc96f",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "In this cell, I run the code above to advance my semantic modeling workflow. It helps me understand how the data is being transformed and prepares it for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b89ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 5 Least Similar Pairs:\")\n",
    "df.sort_values('normalized_similarity', ascending=True).head(5)[['text1', 'text2', 'normalized_similarity']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ca642",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "In this cell, I run the code above to advance my semantic modeling workflow. It helps me understand how the data is being transformed and prepares it for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362d9f2",
   "metadata": {},
   "source": [
    "# üß† Sentence Embedding using Transformers: Analysis and Reflection\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this exercise, I worked with the HuggingFace `transformers` library to generate **sentence embeddings** using the pre-trained model `sentence-transformers/paraphrase-mpnet-base-v2`. Sentence embeddings are numerical representations of text that capture semantic information and can be used for tasks like semantic search, clustering, or sentence similarity.\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1. **Tokenization**  \n",
    "   I used `AutoTokenizer` to tokenize a list of sentences. Tokenization converts text into token IDs while ensuring proper padding and truncation for batch processing.\n",
    "\n",
    "2. **Model Loading**  \n",
    "   I loaded the pre-trained `paraphrase-mpnet-base-v2` model from HuggingFace, which is fine-tuned for generating semantically meaningful sentence embeddings.\n",
    "\n",
    "3. **Model Inference**  \n",
    "   The tokenized input is passed through the transformer model using `torch.no_grad()` to prevent gradient computations, which speeds up inference and reduces memory usage.\n",
    "\n",
    "4. **Mean Pooling**  \n",
    "   After obtaining token-level embeddings from the model, I applied **mean pooling**, which averages the embeddings across all tokens in the sentence, taking the attention mask into account. This provides a fixed-size vector representation per sentence.\n",
    "\n",
    "5. **Output**  \n",
    "   Finally, I printed the sentence embeddings, which are `768`-dimensional vectors capturing the semantic meaning of the input sentences.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Pooling Strategy: Mean Pooling\n",
    "\n",
    "The `mean_pooling` function is crucial here. It ensures that only the actual tokens (not padding) contribute to the final sentence embedding by applying the attention mask. This gives a more accurate representation of the sentence meaning, as padding tokens are excluded from the average.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è If I Had to Build This From Scratch\n",
    "\n",
    "If I were to build a sentence embedding model from scratch (without using a pre-trained transformer), I would approach it in the following way:\n",
    "\n",
    "1. **Data Collection**  \n",
    "   I would first gather a large dataset of sentence pairs with semantic similarity scores, such as the SNLI, STS-B, or Quora Question Pairs datasets.\n",
    "\n",
    "2. **Model Architecture**  \n",
    "   I would implement a transformer-based architecture similar to BERT or MPNet using PyTorch or TensorFlow. This would involve:\n",
    "   - Token and positional embeddings  \n",
    "   - Multi-head self-attention layers  \n",
    "   - Feed-forward layers  \n",
    "   - Layer normalization and residual connections\n",
    "\n",
    "3. **Training Objective**  \n",
    "   I would train the model using a **contrastive learning objective**, such as **triplet loss** or **cosine similarity loss**, to ensure that semantically similar sentences are closer in vector space than dissimilar ones.\n",
    "\n",
    "4. **Pooling Layer**  \n",
    "   After obtaining the final hidden states, I would implement a pooling strategy ‚Äî mean pooling, max pooling, or using the `[CLS]` token embedding ‚Äî to generate fixed-size sentence vectors.\n",
    "\n",
    "5. **Evaluation**  \n",
    "   Finally, I would evaluate the model on downstream tasks like sentence similarity, clustering, or classification using standard benchmarks (e.g., STS Benchmark).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è Reflection\n",
    "\n",
    "Using pre-trained models like `paraphrase-mpnet-base-v2` significantly accelerates NLP experimentation and deployment by providing high-quality semantic representations out of the box. However, understanding the internals ‚Äî like attention, pooling, and training objectives ‚Äî gives me the confidence to customize or build models for specialized applications when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9452b5",
   "metadata": {},
   "source": [
    "# üöÄ Part B: Deployment of Sentence Similarity Model as a Server API Endpoint\n",
    "\n",
    "## Core Approach\n",
    "\n",
    "In Part B, my goal was to **deploy the sentence similarity algorithm developed in Part A as a RESTful API** on a cloud service provider. This API allows external clients to send two input sentences and receive a similarity score in response.\n",
    "\n",
    "### Steps Taken:\n",
    "\n",
    "1. **API Development with FastAPI **  \n",
    "   I wrapped the sentence embedding and similarity computation logic from Part A inside a lightweight web framework:\n",
    "   - The API exposes a POST endpoint `/similarity` that accepts a JSON body with keys `\"text1\"` and `\"text2\"`.\n",
    "   - Upon receiving the request, the API:\n",
    "     - Tokenizes and embeds both input sentences using the pre-trained transformer model.\n",
    "     - Computes the cosine similarity between the two embeddings.\n",
    "     - Returns the similarity score in JSON format with the key `\"similarity score\"`.\n",
    "\n",
    "2. ### **Model Loading and Caching**\n",
    "   - The `SentenceTransformer` model (`paraphrase-mpnet-base-v2`) is either loaded from disk using `joblib` or downloaded and cached on first run.\n",
    "   - This drastically improves performance by preventing the model from reloading with every request.\n",
    "   - The model is used in CPU mode for broad compatibility across environments.\n",
    "\n",
    "\n",
    "3. ### **Hosting**\n",
    "   - The API was deployed on a **self-hosted Linux server** with public IP access.\n",
    "   - It runs on port `8005` and can be accessed globally.\n",
    "   - Deployment was managed using `uvicorn` as the ASGI server.\n",
    "\n",
    "4. **Request-Response Format**  \n",
    "   The API strictly follows the prescribed request-response format:\n",
    "\n",
    "   - **Request JSON**:  \n",
    "     ```json\n",
    "     {\n",
    "       \"text1\": \"nuclear body seeks new tech\",\n",
    "       \"text2\": \"terror suspects face arrest\"\n",
    "     }\n",
    "     ```\n",
    "   \n",
    "   - **Response JSON**:  \n",
    "     ```json\n",
    "     {\n",
    "       \"similarity score\": 0.2\n",
    "     }\n",
    "     ```\n",
    "5. ### **Similarity Computation Logic**\n",
    "   - Both input sentences are encoded into dense vector embeddings using the loaded transformer.\n",
    "   - Cosine similarity is calculated using `sklearn.metrics.pairwise.cosine_similarity`.\n",
    "   - The score, initially in the range `[-1, 1]`, is normalized to `[0, 1]` using the formula:  \n",
    "     \\[\n",
    "     \\text{normalized\\_score} = \\frac{\\text{cosine\\_score} + 1}{2}\n",
    "     \\]\n",
    "   - The output is rounded to 4 decimal places for consistency.\n",
    "\n",
    "6. ### **Error Handling**\n",
    "   - If either `\"text1\"` or `\"text2\"` is missing or empty, a `400 Bad Request` is returned.\n",
    "   - This prevents unnecessary computation and ensures input quality.\n",
    "\n",
    "7. **Testing**  \n",
    "   I tested the API using HTTP clients such as `curl` or Postman to ensure the endpoint correctly processes inputs and returns the expected output.\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Contents\n",
    "\n",
    "- **Live API Endpoint:**  \n",
    "  The deployed API is accessible at:  \n",
    "  `http://207.148.78.17:8005/similarity`  \n",
    "\n",
    "- **Complete Code:**  \n",
    "  Both Part A (model and embedding code) and Part B (API and deployment scripts) are provided as `.py` files.\n",
    "\n",
    "- **Report:**  \n",
    "  This report explains the main approach for both parts in a concise manner.\n",
    "\n",
    "---\n",
    "\n",
    "## Reflection\n",
    "\n",
    "Deploying this sentence similarity model as a cloud-accessible API made it immediately usable for real-world applications. Key strengths of this deployment include:\n",
    "\n",
    "Performance: Using cached model loading and CPU-friendly inference ensured fast response times.\n",
    "\n",
    "Scalability: FastAPI‚Äôs non-blocking async design and uvicorn server allow handling multiple requests concurrently.\n",
    "\n",
    "Portability: The deployment is cloud-agnostic and can easily be ported to AWS, Azure, or Docker environments.\n",
    "\n",
    "However, there are also some limitations to consider:\n",
    "- **Limited Model Flexibility**: Using a pre-trained model limits the model‚Äôs ability to adapt to new data.\n",
    "- **Single Endpoint**: The API only supports a single similarity computation endpoint.\n",
    "- **No Authentication**: The API is open to the public without any authentication or authorization.\n",
    "- **Time Constraints**: The API is not optimized for high-frequency or low-latency use cases and complete model coudn't built in given 2 days timeframe. \n",
    "In summary, this deployment showcases the power of FastAPI and Hugging Face Transformers in building performant, scalable, and portable cloud services.\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
